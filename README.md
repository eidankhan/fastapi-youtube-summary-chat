
# YouTube Transcript Summarizer API (FastAPI + Docker + OpenAI)

## ðŸ“Œ Overview
This project is a **containerized FastAPI application** that:
- Accepts a YouTube transcript and returns a concise AI-generated summary.
- Is modular, allowing future extension (e.g., AI chat feature).
- Uses **OpenAI's GPT models** for summarization.
- Runs fully in **Docker** with optional **Redis** support for chat sessions.

Built with:
- **FastAPI** for modern async API handling
- **OpenAI Python SDK v1.x** for AI integration
- **Docker** + **Docker Compose** for environment consistency
- **Pydantic** for request/response validation

---
## ðŸ“‚ Project Structure

```
fastapi-youtube-summary-chat/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ **init**.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI entry point
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ **init**.py
â”‚   â”‚   â””â”€â”€ openai_client.py    # OpenAI API integration logic
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ **init**.py
â”‚   â”‚   â””â”€â”€ summarize.py        # /summarize endpoint
â”‚   â”œâ”€â”€ schemas.py              # Request/response Pydantic models
â”œâ”€â”€ Dockerfile                  # Container build instructions
â”œâ”€â”€ docker-compose.yml          # Multi-service orchestration (app + redis)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env.example                # Example environment variables (safe to share)
â”œâ”€â”€ .env                        # Actual environment variables (private)
â””â”€â”€ README.md                   # Project documentation
````

## ðŸš€ Step-by-Step Guide

### **1. Prerequisites**
- [Docker Desktop](https://www.docker.com/products/docker-desktop) installed.
- An [OpenAI API Key](https://platform.openai.com/account/api-keys).

---

### **2. Clone the Project**
```bash
git clone https://github.com/your-username/fastapi-youtube-summary-chat.git
cd fastapi-youtube-summary-chat
````

---

### **3. Setup Environment Variables**

1. Copy the example `.env` file:

   ```bash
   cp .env.example .env
   ```
2. Edit `.env` and fill in:

   ```env
   OPENAI_API_KEY=sk-your-openai-key
   OPENAI_MODEL=gpt-4o-mini
   REDIS_URL=redis://redis:6379/0
   ```

**Why two files?**

* `.env.example` â†’ Safe template for collaborators.
* `.env` â†’ Your private API keys and config.

---

### **4. Build and Run with Docker Compose**

```bash
docker compose up --build
```

* `--build` ensures the image is rebuilt from the `Dockerfile`.
* Runs the **FastAPI app** on port `8000` and **Redis** on `6379`.

---

### **5. Verify the API**

Open your browser at:

```
http://localhost:8000/
```

You should see:

```json
{"ok": true, "message": "FastAPI YouTube summarizer + chat"}
```

For API docs (Swagger UI):

```
http://localhost:8000/docs
```

---

### **6. `/summarize` Endpoint Implementation**

#### **Route: `app/routes/summarize.py`**

```python
from fastapi import APIRouter, HTTPException
from app.schemas import SummarizeRequest, SummarizeResponse
from app.core.openai_client import create_summary

router = APIRouter()

@router.post("/summarize", response_model=SummarizeResponse)
async def summarize_endpoint(req: SummarizeRequest):
    if not req.transcript or len(req.transcript.strip()) < 20:
        raise HTTPException(status_code=400, detail="Transcript too short")
    summary = await create_summary(req.transcript, max_tokens=req.max_tokens)
    return SummarizeResponse(summary=summary)
```

#### **OpenAI Client: `app/core/openai_client.py` (v1.x API)**

```python
import os
from openai import OpenAI

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY environment variable is required")

client = OpenAI(api_key=OPENAI_API_KEY)

async def create_summary(transcript: str, max_tokens: int = 300) -> str:
    prompt = (
        "You are a helpful assistant. Read the following YouTube transcript and provide a concise, clear summary "
        "(3-6 sentences). Highlight main points, key conclusions, and any action items if present.\n\n"
        f"Transcript:\n{transcript}\n\nSummary:"
    )

    resp = client.chat.completions.create(
        model=MODEL,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=max_tokens,
        temperature=0.2,
    )
    return resp.choices[0].message.content.strip()
```

### **7. Test the `/summarize` Endpoint**

```bash
curl -X POST http://localhost:8000/api/summarize \
  -H "Content-Type: application/json" \
  -d '{"transcript": "This is a test transcript about the history of Python and why it is loved."}'
```

**Expected Response:**

```json
{"summary": "Python is popular due to its simplicity, readability, and wide applicability..."}
```

## âœ¨ New Feature: Copilot-Style `/chat` API

We have introduced a **single chat endpoint** that mimics Microsoft Copilot behavior â€” allowing both Q&A and summarization in a unified API, while also suggesting follow-up questions to guide further exploration.

### **Endpoint**
`POST /chat`

### **Request Example**
```json
{
  "action": "qa",
  "context": "Transcript or document text here...",
  "question": "What are the key takeaways from this video?",
  "history": []
}
````

* **action** â€” Type of operation (`qa`, `summarize`, etc.)
* **context** â€” The main transcript or content for the model to use.
* **question** â€” Userâ€™s question.
* **history** â€” Previous messages for maintaining context.

---

### **Response Example**

```json
{
  "action": "qa",
  "response": "The key takeaways are...",
  "suggestions": [
    "Explain the long-term risks for Ireland's economy",
    "List the industries most affected by tariffs",
    "Suggest possible strategies for Ireland to adapt"
  ]
}
```

* **response** â€” The AI-generated answer to your question.
* **suggestions** â€” Follow-up questions/options dynamically generated by the AI, similar to Microsoft Copilot.

---

### ðŸ’¡ How it Works

1. We send **context + question** to OpenAI GPT.
2. GPT is instructed to:

   * Answer the question.
   * Generate **3 related follow-up suggestions**.
3. The API returns both in a single response.
4. Frontend can render suggestions as clickable prompts â€” when clicked, send them as a new `question` in the next `/chat` request.

---

### âš ï¸ Token & Cost Consideration

Right now, the **full transcript is sent with each request**.
This ensures context is always available but increases:

* Token usage (higher cost)
* Processing time

**Future Optimization Ideas:**

* Store transcript server-side with a `session_id`
* Use **vector search** to only send relevant transcript chunks

---

### âœ… Example Frontend Flow

1. User asks: `"What are the key takeaways from this video?"`
2. Backend responds with answer + follow-up suggestions.
3. User clicks `"List the industries most affected by tariffs"`.
4. Frontend sends **the same transcript** + new question to `/chat` for another round.

---
# Groq Chat API Integration

This document describes the new **Groq API integration** in our FastAPI project.  
It is designed as an alternative to OpenAI's API, allowing free or cheaper high-quality chat completions using the **Groq Cloud** service.

## Overview

We now support **two AI chat providers**:

1. **OpenAI API** â€“ Paid API, using `OPENAI_API_KEY`.
2. **Groq API** â€“ Free (for now) high-performance API using `GROQ_API_KEY`.

Both APIs follow the same **request** and **response** object structure, making them interchangeable.

## Environment Variables

In your `.env` file, add:

```env
# Groq API
GROQ_API_KEY=your_groq_api_key_here
GROQ_BASE_URL=https://api.groq.com/openai/v1
GROQ_MODEL=llama3-70b-8192

# OpenAI API
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini
OPENAI_BASE_URL=https://api.openai.com/v1
````

## API Endpoint

### POST `/api/chat-groq`

**Request Body:**

```json
{
  "action": "summarize",
  "context": "You are a helpful assistant for summarizing YouTube videos.",
  "question": "Summarize this conversation about AI and UBI.",
  "history": []
}
```

**Request Model:**

```python
class ChatRequest(BaseModel):
    action: str
    context: str
    question: str
    history: list = []
```

## Response

**Example Response:**

```json
{
  "action": "summarize",
  "response": "The discussion focuses on how AI impacts the economy...",
  "suggestions": [
    "What are the pros and cons of universal basic income?",
    "How can humans maintain relevance in an AI-driven world?",
    "What government policies could support AI wealth distribution?"
  ]
}
```

**Response Model:**

```python
class ChatResponse(BaseModel):
    action: str
    response: str
    suggestions: list
```

## Implementation Notes

* The API:

  * Sends context, history, and user question to **Groq API**.
  * Requests the assistant to **return output in strict JSON format** with keys:

    * `answer`
    * `suggestions` (list of strings)
  * Logs raw responses for debugging.
  * Extracts `answer` and `suggestions` from JSON.
  * Falls back to plain text if JSON parsing fails.

## Logging

We log:

* **Raw Groq API responses** for debugging.
* **Warnings** if JSON parsing fails.

Example log snippet:

```
INFO:app.routes.groq_chat:Raw Groq API response: { ... }
WARNING:app.routes.groq_chat:Failed to parse JSON, falling back to plain text.
```

## Usage in Docker

No changes required in existing OpenAI configuration.
Groq service is called directly via API â€“ no local model downloads needed.

# Conversational Chat with Groq + Redis

This feature provides a **persistent conversational interface** with Groqâ€™s LLaMA model (`llama3-70b-8192` by default), allowing:
- Context-aware question answering (`qa`).
- Automatic summarization of content (`summary`).
- Content expansion/explanation (`expand`).
- Session persistence across requests.
- Automatic token-limit handling to prevent `413 Request Too Large` errors.


## Architecture
The system is composed of:
- **Frontend**: Sends requests with a `sessionId` to preserve conversation history.
- **Backend** (`service.py`): Orchestrates history retrieval, model calls, and response formatting.
- **Redis**: Stores chat history and metadata with expiration.
- **Groq API**: Processes requests using the OpenAI-compatible endpoint.

## How It Works

### 1. Session Handling
- Each conversation has a unique `sessionId` (UUID).
- If no `sessionId` is sent from the frontend, the backend creates one.
- Metadata (`created_at`) and messages are stored in Redis with a **TTL** (time-to-live).

### 2. Message Storage
Messages are appended in JSON format:

```json
{ "role": "user", "content": "Hello" }
{ "role": "assistant", "content": "Hi there!" }
````

History is automatically trimmed to a maximum (`MAX_HISTORY_MESSAGES`).

### 3. Token Limit Handling

Before sending messages to Groq, the history is **token-trimmed** using `tiktoken` to avoid exceeding Groqâ€™s token per minute (TPM) limits.

### 4. Supported Actions

* **qa** â€” Answer a user question using context.
* **summary** â€” Summarize given content.
* **expand** â€” Expand and elaborate on given content.

### 5. Model Output

Expected in **JSON** format:

```json
{
  "answer": "Main reply text.",
  "suggestions": ["Follow-up 1", "Follow-up 2", "Follow-up 3"]
}
```

If parsing fails, raw text is returned with empty suggestions.


## Request Flow Diagram

```plaintext
+-----------+       +------------+       +--------+       +-----------+
|  Frontend | ----> |  Backend   | ----> | Redis  |       |   Groq    |
| (Browser) |       | (FastAPI)  |       | Cache  |       |   API     |
+-----------+       +------------+       +--------+       +-----------+
     |                    |                  |                 |
     | Send POST /chat    |                  |                 |
     | with sessionId     |                  |                 |
     |------------------->|                  |                 |
     |                    | Get history ---->|                 |
     |                    |<-----------------|                 |
     |                    | Append new msg   |                 |
     |                    |----------------->|                 |
     |                    | Call LLaMA model |---------------> |
     |                    |<-----------------------------------|
     |                    | Save assistant msg in Redis        |
     |                    |----------------->|                 |
     | Return JSON result |                  |                 |
     |<-------------------|                  |                 |
```


## Environment Variables

Add these to `.env`:

```env
GROQ_API_KEY=your_api_key_here
GROQ_BASE_URL=https://api.groq.com/openai/v1
GROQ_MODEL=llama3-70b-8192

REDIS_URL=redis://redis:6379/0
GROQ_REDIS_PREFIX=groq_session_
SESSION_TTL_SECONDS=3600
```

## Frontend Usage

### Example: Summary

```javascript
const sessionId = localStorage.getItem("sessionId") || crypto.randomUUID();
localStorage.setItem("sessionId", sessionId);

await fetch(CHAT_API_URL, {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    sessionId,
    action: "summary",
    context: transcriptText,
    question: "Summarize this YouTube video"
  })
});
```

### Example: Follow-up Question

```javascript
await fetch(CHAT_API_URL, {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    sessionId,
    action: "qa",
    context: transcriptData.transcript,
    question
  })
});
```

## Redis Keys

```
groq_session_<sessionId>
groq_session_<sessionId>:meta
```

* `meta` contains session metadata (e.g., `created_at`).
* Main key stores an array of JSON messages.

## Error Handling

* **Missing API Key** â€” Logs warning and fails at model call.
* **Token Overflow (413)** â€” Handled by trimming history before sending.
* **Invalid JSON from Model** â€” Falls back to plain text with no suggestions.

```